a1.sources = r1
a1.channels = c1
a1.sinks = s1

a1.sources.r1.type = TAILDIR
# positionFile 检查点文件路径
a1.sources.r1.positionFile = /usr/local/flume/data/taildir_position.json
# 指定filegroups，可以有多个，以空格分隔；（TailSource可以同时监控tail多个目录中的文件）
a1.sources.r1.filegroups = f1
# 指定监控的文件目录f1,匹配的文件信息
a1.sources.r1.filegroups.f1 = /application/logs/info.*.log
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = timestamp

#  sins1 配置
a1.sinks.s1.channel = c1
a1.sinks.s1.type = org.apache.flume.sink.kafka.KafkaSink
# kafka主题队列
a1.sinks.s1.kafka.topic = bill_topic
# kafka服务端口
a1.sinks.s1.kafka.bootstrap.servers = 10.2.9.150:9092
# flume批次发送数量
a1.sinks.s1.kafka.flumeBatchSize = 20
# kafka确认机制，1是leader接收成功后返回确认
a1.sinks.s1.kafka.producer.acks = 1
# 如果设置了该时间，本该即使发送的消息需要等待这个时间内加入批次后，再发送
a1.sinks.s1.kafka.producer.linger.ms = 1
# 压缩类型
a1.sinks.s1.kafka.producer.compression.type = snappy


# fileChannel 配置
a1.channels.c1.type = file
# 检测点文件所存储的目录
a1.channels.c1.checkpointDir = /usr/local/flume/checkpoint
# 数据存储所在的目录设置
a1.channels.c1.dataDirs = /usr/local/flume/data
# 隧道的最大容量
a1.channels.c1.capacity = 10000
# 事务容量的最大值设置
a1.channels.c1.transactionCapacity = 200

#用channel链接source和sink
a1.sources.r1.channels = c1
a1.sinks.s1.channel = c1